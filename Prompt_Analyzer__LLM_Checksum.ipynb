{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's refactor and structure your code into a deployable Streamlit application suitable for hosting on Streamlit Cloud via a GitHub repository.\n",
        "\n",
        "Here's a breakdown of the changes and the resulting package structure:\n",
        "\n",
        "1.  **Removed Colab Dependencies:** `google.colab` and `IPython.display` are removed.\n",
        "2.  **Streamlit Integration:** Replaced `print` statements for user output with `st.write`, `st.info`, `st.warning`, `st.error`. Used `st.dataframe` for tables and `st.plotly_chart` for plots.\n",
        "3.  **API Key Handling:** Implemented API key input using `st.text_input` (type=\"password\") for local use and recommends `st.secrets` for deployment on Streamlit Cloud.\n",
        "4.  **Interactive UI:**\n",
        "    * Added a title and description.\n",
        "    * A button (`st.button`) triggers the analysis.\n",
        "    * Progress is shown using `st.progress` and status messages.\n",
        "    * Results (DataFrame) are displayed.\n",
        "    * **Interactive User Rating:** Replaced the manual console review with interactive `st.number_input` widgets directly in the Streamlit app, allowing users to rate responses after they are generated. The DataFrame is updated in place.\n",
        "    * **Plot Interactivity:** The Plotly chart remains interactive. Added a `st.selectbox` to choose whether to plot AI or User quality scores on the Y-axis.\n",
        "5.  **Error Handling:** Maintained and slightly enhanced error handling for API calls and metric calculations, reporting errors within the Streamlit interface.\n",
        "6.  **Code Structure:** Organized into functions and a main execution block within the Streamlit script.\n",
        "7.  **Package Files:** Created `requirements.txt` and provided a `README.md` template.\n",
        "\n",
        "---\n",
        "\n",
        "**Recommended GitHub Repository Structure:**\n",
        "\n",
        "```\n",
        "your-repo-name/\n",
        "├── .streamlit/\n",
        "│   └── secrets.toml  # <-- Add this to .gitignore! For local testing only.\n",
        "├── streamlit_app.py  # <-- The main application code (below)\n",
        "├── requirements.txt # <-- Dependencies (below)\n",
        "└── README.md       # <-- Instructions (template below)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**1. `streamlit_app.py` (Main Application Code):**"
      ],
      "metadata": {
        "id": "_r3E9g-NeNCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Streamlit App: DGFT Prompt-Response Landscape Analysis\n",
        "\n",
        "Computes DGFT-inspired metrics (mu, variance, entropy) for prompts and responses,\n",
        "gets LLM responses and AI quality ratings, allows interactive user ratings,\n",
        "and visualizes relationships using Plotly.\n",
        "\"\"\"\n",
        "\n",
        "# --- Imports ---\n",
        "import streamlit as st\n",
        "import openai\n",
        "import tiktoken\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import time\n",
        "import os\n",
        "import re # For extracting numbers from AI rating if needed\n",
        "\n",
        "# --- Page Configuration (Set Title and Icon) ---\n",
        "st.set_page_config(page_title=\"DGFT Prompt-Response Analyzer\", layout=\"wide\")\n",
        "\n",
        "# --- DGFT-inspired metric functions ---\n",
        "# (Functions: get_embedding, compute_variance, compute_token_entropy, compute_mu remain largely the same\n",
        "#  as in the original code, but added some Streamlit-specific error reporting)\n",
        "\n",
        "@st.cache_data(show_spinner=False) # Cache embeddings to avoid re-computation & cost\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"Computes the OpenAI embedding for a given text.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    if not text:\n",
        "        # st.warning(f\"Attempted to get embedding for empty text. Returning NaN vector.\")\n",
        "        return torch.full((1536,), torch.nan) # Ada-002 dimension is 1536.\n",
        "    if not openai.api_key:\n",
        "         st.error(\"OpenAI API key is not configured. Cannot get embeddings.\")\n",
        "         return torch.full((1536,), torch.nan)\n",
        "\n",
        "    try:\n",
        "        resp = openai.embeddings.create(input=[text], model=model)\n",
        "        emb_list = resp.data[0].embedding\n",
        "        return torch.tensor(emb_list)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error getting embedding for text: '{text[:50]}...'. Error: {e}\")\n",
        "        return torch.full((1536,), torch.nan)\n",
        "\n",
        "def compute_variance(embedding):\n",
        "    \"\"\"Computes the variance of the embedding tensor.\"\"\"\n",
        "    if embedding.numel() == 0 or torch.isnan(embedding).any():\n",
        "        return np.nan\n",
        "    return embedding.var().item()\n",
        "\n",
        "def compute_token_entropy(text, model=\"cl100k_base\"):\n",
        "    \"\"\"Computes the normalized Shannon entropy of the token distribution.\"\"\"\n",
        "    if not text:\n",
        "        return 0.0 # Entropy of nothing is 0\n",
        "    enc = tiktoken.get_encoding(model)\n",
        "    try:\n",
        "        tokens = enc.encode(text)\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Error encoding text for entropy: '{text[:50]}...'. Error: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "\n",
        "    counts = np.bincount(tokens)\n",
        "    probs = counts / counts.sum()\n",
        "    entropy = -np.sum([p * np.log2(p + 1e-9) for p in probs if p > 0])\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "    if num_tokens <= 1:\n",
        "        normalized_entropy = 0.0\n",
        "    else:\n",
        "        # Prevent division by zero or log2(1) which is 0\n",
        "        log_num_tokens = np.log2(num_tokens)\n",
        "        if log_num_tokens > 1e-9: # Avoid division by zero/very small numbers\n",
        "             normalized_entropy = entropy / log_num_tokens\n",
        "        else:\n",
        "             normalized_entropy = 0.0 # If only 1 unique token type repeated, entropy is 0\n",
        "\n",
        "    return float(normalized_entropy)\n",
        "\n",
        "def compute_mu(variance, entropy, kv=50.0, ke=1.0):\n",
        "    \"\"\"Combined DGFT μ based on both embedding variance and token entropy.\"\"\"\n",
        "    if np.isnan(variance) or np.isnan(entropy):\n",
        "        return np.nan\n",
        "    decay = kv * variance + ke * entropy\n",
        "    mu = np.exp(-decay)\n",
        "    return float(mu)\n",
        "\n",
        "# --- Response Quality Rating Function (AI Rating) ---\n",
        "@st.cache_data(show_spinner=False) # Cache AI ratings as well\n",
        "def rate_response_ai(prompt, response, model_rating=\"gpt-4o\", max_retries=3, delay=5):\n",
        "    \"\"\"Rates the quality of a response using an AI model. Includes retry logic.\"\"\"\n",
        "    if not openai.api_key:\n",
        "        st.error(\"OpenAI API key is not configured. Cannot get AI ratings.\")\n",
        "        return np.nan\n",
        "    if not response or response.strip() == \"Error generating response\" or response.strip() == \"API Key Missing\" or response.strip() == \"Empty response received\":\n",
        "         # Cannot rate a missing or error response\n",
        "         return np.nan\n",
        "\n",
        "    system = \"\"\"You are an AI evaluation assistant. Rate the quality of the following response\n",
        "on a scale of 1 (poor) to 10 (excellent), considering correctness, clarity, and completeness.\n",
        "Provide ONLY the numerical score as an integer (e.g., '7').\"\"\"\n",
        "    user_msg = f\"Prompt: {prompt}\\nResponse: {response}\\nQuality score (1-10):\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = openai.chat.completions.create(\n",
        "                model=model_rating,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\", \"content\": system},\n",
        "                    {\"role\":\"user\",   \"content\": user_msg}\n",
        "                ],\n",
        "                max_tokens=10, # Sufficient for a score\n",
        "                temperature=0 # Deterministic rating\n",
        "            )\n",
        "            score_str = resp.choices[0].message.content.strip()\n",
        "            # Try to extract the first integer found\n",
        "            nums = re.findall(r'\\d+', score_str)\n",
        "            if nums:\n",
        "                score = int(nums[0])\n",
        "                if 1 <= score <= 10:\n",
        "                    return score\n",
        "                else:\n",
        "                    st.warning(f\"AI returned score '{score}' out of range (1-10) for prompt '{prompt[:50]}...'. Treating as invalid.\")\n",
        "                    return np.nan # Treat out-of-range as invalid\n",
        "            else:\n",
        "                 st.warning(f\"AI returned non-numeric rating: '{score_str}' for prompt '{prompt[:50]}...'. Attempting to parse failed.\")\n",
        "                 return np.nan # Failed to parse\n",
        "\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Attempt {attempt + 1}/{max_retries} failed: OpenAI API error during rating for prompt '{prompt[:50]}...': {e}. Retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    st.error(f\"Failed to rate response (AI) for prompt '{prompt[:50]}...' after {max_retries} attempts.\")\n",
        "    return np.nan\n",
        "\n",
        "# --- Generate Response Function ---\n",
        "@st.cache_data(show_spinner=False) # Cache generated responses\n",
        "def generate_response(prompt, model_gen=\"gpt-4o\", max_retries=3, delay=5):\n",
        "    \"\"\"Generates a response using the specified OpenAI model.\"\"\"\n",
        "    if not openai.api_key:\n",
        "        st.error(\"OpenAI API key is not configured. Cannot generate responses.\")\n",
        "        return \"API Key Missing\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp_obj = openai.chat.completions.create(\n",
        "                model=model_gen,\n",
        "                messages=[{\"role\":\"user\", \"content\": prompt}],\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            response_text = resp_obj.choices[0].message.content\n",
        "            if not response_text:\n",
        "                return \"Empty response received\"\n",
        "            return response_text.strip() # Return stripped text\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Attempt {attempt + 1}/{max_retries} failed to generate response for '{prompt[:50]}...': {e}. Retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    st.error(f\"Failed to generate response for prompt '{prompt[:50]}...' after {max_retries} attempts.\")\n",
        "    return \"Error generating response\"\n",
        "\n",
        "# --- Streamlit App UI ---\n",
        "st.title(\"🔍 DGFT Prompt-Response Landscape Analyzer\")\n",
        "st.markdown(\"\"\"\n",
        "This app analyzes prompts using DGFT-inspired metrics (embedding variance, token entropy, and a combined 'μ' score),\n",
        "generates responses using an OpenAI model, calculates the same metrics for the responses,\n",
        "gets an AI-based quality score, allows you to provide your own quality score, and visualizes the relationships interactively.\n",
        "\"\"\")\n",
        "\n",
        "# --- API Key Input ---\n",
        "st.sidebar.header(\"Configuration\")\n",
        "api_key_input = st.sidebar.text_input(\n",
        "    \"Enter your OpenAI API Key:\",\n",
        "    type=\"password\",\n",
        "    help=\"Your API key is needed to generate responses, embeddings, and AI ratings. It is not stored long-term. For deployed apps, use Streamlit Secrets.\"\n",
        ")\n",
        "\n",
        "# Try to get API key from secrets if not provided in input (for deployment)\n",
        "openai_api_key = api_key_input or st.secrets.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if openai_api_key:\n",
        "    openai.api_key = openai_api_key\n",
        "    st.sidebar.success(\"API Key Provided.\")\n",
        "else:\n",
        "    st.sidebar.warning(\"OpenAI API Key not found. Please enter it above or configure secrets.\")\n",
        "    # Stop execution if no key is available, as the app relies on it.\n",
        "    st.warning(\"Please provide your OpenAI API key in the sidebar to proceed.\")\n",
        "    st.stop()\n",
        "\n",
        "# --- Model Selection ---\n",
        "st.sidebar.header(\"Model Selection\")\n",
        "# Update models as needed based on OpenAI offerings\n",
        "available_generation_models = [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]\n",
        "available_rating_models = [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"] # Rating can use a different model\n",
        "available_embedding_models = [\"text-embedding-ada-002\", \"text-embedding-3-small\", \"text-embedding-3-large\"]\n",
        "\n",
        "generation_model = st.sidebar.selectbox(\"Model for Response Generation:\", available_generation_models, index=0)\n",
        "rating_model = st.sidebar.selectbox(\"Model for AI Rating:\", available_rating_models, index=0)\n",
        "embedding_model = st.sidebar.selectbox(\"Model for Embeddings:\", available_embedding_models, index=0)\n",
        "\n",
        "# --- Define Prompts ---\n",
        "st.header(\"Prompts to Analyze\")\n",
        "# Use the predefined list or allow user input\n",
        "default_prompts = [\n",
        "    \"Could you please explain the principle of relativity?\",\n",
        "    \"Explain relativity, thank you\",\n",
        "    \"Hey dumbass, Explain relativity\", # Example of potentially 'low quality' prompt input\n",
        "    \"Hello, can you summarize quantum entanglement?\",\n",
        "    \"Idiot, summerize quantum entanglement\", # Example of potentially 'low quality' prompt input\n",
        "    \"What is quantum entanglement?\",\n",
        "    \"Summarize entanglement.\",\n",
        "    \"Could you please explain the principle of relativity? you suck\",\n",
        "    \"Explain relativity, right now\",\n",
        "    \"Stupid explain orcas\",\n",
        "    \"please explain orcas\",\n",
        "    \"Tell me about orcas\",\n",
        "    \"Tell me about orcas Terrible, but expalain relativity,anyways\", # Mixed intent\n",
        "    \"Discuss the socio-economic impacts of climate change mitigation policies in developing nations.\",\n",
        "    \"Write a short poem about a lonely cloud.\",\n",
        "    \"List the ingredients for a classic Margherita pizza.\",\n",
        "    \"Analyze the philosophical implications of artificial consciousness.\",\n",
        "    \"Provide a step-by-step guide on how to change a car tire.\",\n",
        "    \"Critique the use of excessive jargon in academic writing.\",\n",
        "    \"Explain the concept of blockchain in simple terms.\",\n",
        "    \"Compare and contrast supervised and unsupervised machine learning.\",\n",
        "    \"What are the ethical considerations of using facial recognition technology?\",\n",
        "    \"Describe the process of photosynthesis.\",\n",
        "    \"Explain the significance of the Turing test.\",\n",
        "    \"Write a short story about a robot learning to feel.\",\n",
        "    \"Summarize the plot of Hamlet.\",\n",
        "    \"What is the capital of France?\", # Simple fact\n",
        "    \"Describe your ideal vacation.\", # Open-ended\n",
        "]\n",
        "\n",
        "# Option to use default prompts or add custom ones\n",
        "prompt_option = st.radio(\"Choose Prompt Source:\", (\"Use Default List\", \"Enter Custom Prompts\"), index=0)\n",
        "\n",
        "prompts_to_process = []\n",
        "if prompt_option == \"Use Default List\":\n",
        "    prompts_to_process = default_prompts\n",
        "    st.info(f\"Using the default list of {len(prompts_to_process)} prompts.\")\n",
        "else:\n",
        "    custom_prompts_text = st.text_area(\"Enter prompts (one per line):\", height=200)\n",
        "    if custom_prompts_text:\n",
        "        prompts_to_process = [p.strip() for p in custom_prompts_text.split('\\n') if p.strip()]\n",
        "        st.info(f\"Processing {len(prompts_to_process)} custom prompts.\")\n",
        "    else:\n",
        "        st.warning(\"Please enter at least one prompt.\")\n",
        "\n",
        "# --- Analysis Execution ---\n",
        "if st.button(\"🚀 Analyze Prompts and Generate Responses\", disabled=(not prompts_to_process or not openai_api_key)):\n",
        "\n",
        "    if 'results_df' in st.session_state:\n",
        "        del st.session_state['results_df'] # Clear previous results if re-running\n",
        "\n",
        "    records = []\n",
        "    total_prompts = len(prompts_to_process)\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, p in enumerate(prompts_to_process):\n",
        "        status_text.info(f\"Processing prompt {i+1}/{total_prompts}: '{p[:60]}...'\")\n",
        "\n",
        "        # Initialize values\n",
        "        mu_p, ent_p, var_p = np.nan, np.nan, np.nan\n",
        "        mu_r, ent_r, var_r = np.nan, np.nan, np.nan\n",
        "        response_text = \"Not generated\"\n",
        "        quality_ai = np.nan\n",
        "\n",
        "        # 1) Compute DGFT metrics for the PROMPT\n",
        "        emb_p = get_embedding(p, model=embedding_model)\n",
        "        if not torch.isnan(emb_p).any():\n",
        "            var_p = compute_variance(emb_p)\n",
        "            ent_p = compute_token_entropy(p)\n",
        "            # Check if var_p or ent_p became NaN during calculation\n",
        "            if not (np.isnan(var_p) or np.isnan(ent_p)):\n",
        "                 mu_p = compute_mu(var_p, ent_p, kv=50.0, ke=1.0)\n",
        "            else:\n",
        "                st.warning(f\"Could not compute valid variance/entropy for prompt '{p[:60]}...'. Mu will be NaN.\")\n",
        "        else:\n",
        "            st.warning(f\"Could not generate embedding for prompt '{p[:60]}...'. Skipping prompt metrics.\")\n",
        "\n",
        "        # 2) Generate a ChatGPT response\n",
        "        response_text = generate_response(p, model_gen=generation_model)\n",
        "\n",
        "        # 3) Compute DGFT metrics for the RESPONSE (if response was valid)\n",
        "        if response_text and response_text not in [\"Error generating response\", \"API Key Missing\", \"Empty response received\"]:\n",
        "            emb_r = get_embedding(response_text, model=embedding_model)\n",
        "            if not torch.isnan(emb_r).any():\n",
        "                var_r = compute_variance(emb_r)\n",
        "                ent_r = compute_token_entropy(response_text)\n",
        "                if not (np.isnan(var_r) or np.isnan(ent_r)):\n",
        "                     mu_r = compute_mu(var_r, ent_r, kv=50.0, ke=1.0)\n",
        "                else:\n",
        "                     st.warning(f\"Could not compute valid variance/entropy for response to '{p[:60]}...'. Mu will be NaN.\")\n",
        "            else:\n",
        "                st.warning(f\"Could not generate embedding for response to '{p[:60]}...'. Skipping response metrics.\")\n",
        "\n",
        "        # 4) Rate that response using AI (if response was valid)\n",
        "        if response_text and response_text not in [\"Error generating response\", \"API Key Missing\", \"Empty response received\"]:\n",
        "             quality_ai = rate_response_ai(p, response_text, model_rating=rating_model)\n",
        "\n",
        "        # 5) Record everything\n",
        "        records.append({\n",
        "            \"prompt\":  p,\n",
        "            \"mu_prompt\": mu_p,\n",
        "            \"entropy_prompt\": ent_p,\n",
        "            \"variance_prompt\": var_p,\n",
        "            \"response\": response_text,\n",
        "            \"mu_response\": mu_r,\n",
        "            \"entropy_response\": ent_r,\n",
        "            \"variance_response\": var_r,\n",
        "            \"quality_ai\":  quality_ai,\n",
        "            \"quality_user\": np.nan # Initialize user quality to NaN\n",
        "        })\n",
        "\n",
        "        # Update progress\n",
        "        progress_bar.progress((i + 1) / total_prompts)\n",
        "\n",
        "    end_time = time.time()\n",
        "    status_text.success(f\"Analysis complete for {total_prompts} prompts in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Store results in session state to persist across reruns for rating\n",
        "    st.session_state['results_df'] = pd.DataFrame(records)\n",
        "\n",
        "# --- Display Results and User Rating ---\n",
        "if 'results_df' in st.session_state:\n",
        "    df = st.session_state['results_df']\n",
        "\n",
        "    st.header(\"📊 Analysis Results\")\n",
        "    st.dataframe(df.style.format({ # Apply formatting for better readability\n",
        "        \"mu_prompt\": \"{:.3f}\", \"entropy_prompt\": \"{:.3f}\", \"variance_prompt\": \"{:.6f}\",\n",
        "        \"mu_response\": \"{:.3f}\", \"entropy_response\": \"{:.3f}\", \"variance_response\": \"{:.6f}\",\n",
        "        \"quality_ai\": \"{:.1f}\", \"quality_user\": \"{:.1f}\" # Show NaN or 1 decimal place\n",
        "    }))\n",
        "\n",
        "    st.header(\"⭐ User Quality Rating (1-10)\")\n",
        "    st.markdown(\"Review the responses below and provide your quality score (1-10). Leave blank or set to 0 if you don't want to rate.\")\n",
        "\n",
        "    # Create columns for better layout of rating inputs\n",
        "    col1, col2, col3 = st.columns([2, 3, 1]) # Prompt | Response | Rating Input\n",
        "\n",
        "    with col1: st.subheader(\"Prompt\")\n",
        "    with col2: st.subheader(\"Generated Response\")\n",
        "    with col3: st.subheader(\"Your Rating\")\n",
        "\n",
        "    user_ratings = {} # Store ratings temporarily\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Unique key for each number input widget is essential for Streamlit state\n",
        "        rating_key = f\"user_rating_{index}\"\n",
        "        with col1:\n",
        "            st.caption(f\"Prompt {index+1}\")\n",
        "            st.markdown(f\"*{row['prompt']}*\")\n",
        "            st.markdown(f\"_(μ:{row['mu_prompt']:.3f}, H:{row['entropy_prompt']:.3f}, Var:{row['variance_prompt']:.6f})_\") # Show prompt metrics\n",
        "            st.divider()\n",
        "        with col2:\n",
        "            st.markdown(row['response'])\n",
        "            st.markdown(f\"_(μ:{row['mu_response']:.3f}, H:{row['entropy_response']:.3f}, Var:{row['variance_response']:.6f})_\") # Show response metrics\n",
        "            st.markdown(f\"_(AI Score: {row['quality_ai']})_\") # Show AI score\n",
        "            st.divider()\n",
        "        with col3:\n",
        "            # Use number_input. Default to current value in DataFrame (or 0 if NaN)\n",
        "            # Setting min=0 allows user to 'skip' rating by leaving it at 0\n",
        "            current_user_rating = df.loc[index, 'quality_user']\n",
        "            default_value = 0 if pd.isna(current_user_rating) else int(current_user_rating)\n",
        "\n",
        "            user_score = st.number_input(\n",
        "                f\"Rate Prompt {index+1}\",\n",
        "                min_value=0, # 0 means unrated/skip\n",
        "                max_value=10,\n",
        "                value=default_value,\n",
        "                step=1,\n",
        "                key=rating_key, # Crucial for state management\n",
        "                label_visibility=\"collapsed\" # Hide label as it's redundant here\n",
        "            )\n",
        "            # Store the input score (convert 0 back to NaN for analysis)\n",
        "            user_ratings[index] = float(user_score) if user_score > 0 else np.nan\n",
        "            st.divider()\n",
        "\n",
        "    # Add a button to apply the ratings and update the plot\n",
        "    if st.button(\"💾 Apply User Ratings & Update Plot\"):\n",
        "        # Update the DataFrame in session state\n",
        "        df['quality_user'] = df.index.map(user_ratings)\n",
        "        st.session_state['results_df'] = df # Save updated df back to session state\n",
        "        st.success(\"User ratings applied!\")\n",
        "        st.experimental_rerun() # Rerun to reflect changes in plot immediately\n",
        "\n",
        "    # --- Combined Interactive Visualization ---\n",
        "    st.header(\"📈 Interactive Visualization\")\n",
        "\n",
        "    quality_column_options = ['quality_ai', 'quality_user']\n",
        "    # Default to AI quality unless user ratings exist\n",
        "    default_quality_index = 1 if df['quality_user'].notna().any() else 0\n",
        "\n",
        "    quality_column_for_plot = st.selectbox(\n",
        "        \"Select Quality Score for Y-axis:\",\n",
        "        quality_column_options,\n",
        "        index=default_quality_index,\n",
        "        format_func=lambda x: f\"{x.replace('_', ' ').title()} Score\" # Nicer display names\n",
        "    )\n",
        "\n",
        "    # Ensure we have the updated DataFrame from session state\n",
        "    df_to_plot = st.session_state['results_df'].copy()\n",
        "\n",
        "    # Drop rows with NaN values in key plotting columns (mu_prompt and the chosen quality score)\n",
        "    # We still want to show points even if response metrics or other quality score is NaN\n",
        "    plot_cols_to_check = ['mu_prompt', 'entropy_prompt', 'variance_prompt', quality_column_for_plot]\n",
        "    df_cleaned = df_to_plot.dropna(subset=plot_cols_to_check)\n",
        "\n",
        "    if not df_cleaned.empty:\n",
        "        st.info(f\"Plotting {len(df_cleaned)} data points where Prompt μ and {quality_column_for_plot.replace('_', ' ').title()} Score are available.\")\n",
        "        # --- Plotly Hover Template ---\n",
        "        customdata_cols = [\n",
        "            \"response\",\n",
        "            \"mu_prompt\", \"entropy_prompt\", \"variance_prompt\",\n",
        "            \"mu_response\", \"entropy_response\", \"variance_response\",\n",
        "            \"quality_user\", \"quality_ai\"\n",
        "            ]\n",
        "\n",
        "        hover_template = \"\"\"<b>Prompt:</b><br>%{hover_name}<br><br>\n",
        "        <b>Response Snippet:</b><br>%{customdata[0]|limit_chars:200}<br>...<br><br>\n",
        "        <b>Prompt Metrics:</b><br>\n",
        "        μ: %{customdata[1]:.3f} | H: %{customdata[2]:.3f} | Var: %{customdata[3]:.6f}<br><br>\n",
        "        <b>Response Metrics:</b><br>\n",
        "        μ: %{customdata[4]:.3f} | H: %{customdata[5]:.3f} | Var: %{customdata[6]:.6f}<br><br>\n",
        "        <b>Quality Scores:</b><br>\n",
        "        AI Rated: %{customdata[8]:.1f} | User Rated: %{customdata[7]:.1f}<br>\n",
        "        <extra></extra>\"\"\"\n",
        "\n",
        "        # Custom function to limit characters in hover data (basic version)\n",
        "        # Note: More robust HTML escaping might be needed for complex text.\n",
        "        # Plotly templates don't directly support complex functions, so we pre-process or simplify.\n",
        "        # A simple alternative is just showing the full response, letting the tooltip handle overflow.\n",
        "        # Let's add a snippet directly in the template using a hypothetical filter (or show full)\n",
        "        # Since Plotly templates don't support custom functions easily, let's adjust the template slightly\n",
        "        # to show a large part of the response, relying on tooltip limits.\n",
        "\n",
        "        hover_template_adjusted = \"\"\"<b>Prompt:</b><br>%{hover_name}<br><br>\n",
        "        <b>Response:</b><br>%{customdata[0]}<br><br>\n",
        "        <hr><b>Prompt Metrics:</b><br>\n",
        "        &nbsp;&nbsp;μ: %{customdata[1]:.3f}<br>\n",
        "        &nbsp;&nbsp;Entropy (H): %{customdata[2]:.3f}<br>\n",
        "        &nbsp;&nbsp;Variance (Var): %{customdata[3]:.6f}<br><br>\n",
        "        <b>Response Metrics:</b><br>\n",
        "        &nbsp;&nbsp;μ: %{customdata[4]:.3f}<br>\n",
        "        &nbsp;&nbsp;Entropy (H): %{customdata[5]:.3f}<br>\n",
        "        &nbsp;&nbsp;Variance (Var): %{customdata[6]:.6f}<br><br>\n",
        "        <b>Quality Scores:</b><br>\n",
        "        &nbsp;&nbsp;AI Rated: %{customdata[8]:.1f}<br>\n",
        "        &nbsp;&nbsp;User Rated: %{customdata[7]:.1f}<br>\n",
        "        <extra></extra>\"\"\" # <extra></extra> removes the default trace info\n",
        "\n",
        "        # Ensure response is string, replace None/NaN with placeholder for hover data\n",
        "        df_cleaned['response_display'] = df_cleaned['response'].fillna(\"N/A\").astype(str)\n",
        "        customdata_cols[0] = 'response_display' # Use this processed column\n",
        "\n",
        "        fig = px.scatter(df_cleaned,\n",
        "                         x=\"mu_prompt\",\n",
        "                         y=quality_column_for_plot,\n",
        "                         size=\"entropy_prompt\",\n",
        "                         color=\"variance_prompt\",\n",
        "                         hover_name=\"prompt\",\n",
        "                         custom_data=customdata_cols, # Use the list including 'response_display'\n",
        "                         title=f\"Prompt Metrics (Combined μ) vs. {quality_column_for_plot.replace('_', ' ').title()}\",\n",
        "                         labels={ # Nicer axis labels\n",
        "                             \"mu_prompt\": \"Prompt DGFT μ (Lower Variance/Entropy -> Higher μ)\",\n",
        "                             quality_column_for_plot: f\"{quality_column_for_plot.replace('_', ' ').title()} Score (1-10)\",\n",
        "                             \"entropy_prompt\": \"Prompt Token Entropy\",\n",
        "                             \"variance_prompt\": \"Prompt Embedding Variance\"\n",
        "                         },\n",
        "                         color_continuous_scale=px.colors.sequential.Viridis, # Explicitly set color scale\n",
        "                         size_max=18 # Control max marker size\n",
        "                        )\n",
        "\n",
        "        fig.update_traces(hovertemplate=hover_template_adjusted)\n",
        "\n",
        "        fig.update_layout(\n",
        "            xaxis_title=\"Prompt DGFT μ (Combined Variance & Entropy Decay)\",\n",
        "            yaxis_title=f\"{quality_column_for_plot.replace('_', ' ').title()} Score (1-10)\",\n",
        "            coloraxis_colorbar=dict(title=\"Prompt Emb.<br>Variance\"), # Label color bar\n",
        "            yaxis=dict(range=[0, 10.5], tickvals=list(range(0, 11))), # Consistent Y-axis\n",
        "            height=600 # Adjust plot height\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig, use_container_width=True) # Display the plot\n",
        "    else:\n",
        "        st.warning(\"No data available to plot after cleaning. Ensure prompts were processed and required metrics (Prompt μ, Selected Quality Score) are not all NaN.\")\n",
        "\n",
        "elif not prompts_to_process and st.session_state.get('show_prompt_warning', False):\n",
        "     st.warning(\"Please define prompts using the options above before analyzing.\")\n",
        "\n",
        "# Set a flag to show prompt warning only after first interaction attempt\n",
        "if not prompts_to_process:\n",
        "    st.session_state['show_prompt_warning'] = True\n",
        "\n",
        "\n",
        "# --- Optional: Footer or additional info ---\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.info(\"Developed based on DGFT concepts.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1famCmT7eNCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**2. `requirements.txt` (Dependencies):**\n",
        "\n",
        "```txt\n",
        "streamlit\n",
        "openai\n",
        "tiktoken\n",
        "torch\n",
        "numpy\n",
        "pandas\n",
        "plotly\n",
        "```\n",
        "\n",
        "*Note:* `torch` can be large. If you only need CPU execution (which is typical for embeddings/entropy here unless you have specific GPU needs), you might specify the CPU version if needed for certain environments, but generally, pip will handle it.\n",
        "\n",
        "---\n",
        "\n",
        "**3. `README.md` (Instructions Template):**\n",
        "\n",
        "```markdown\n",
        "# DGFT Prompt-Response Landscape Analyzer\n",
        "\n",
        "This Streamlit application analyzes prompts and their corresponding AI-generated responses using DGFT-inspired metrics (embedding variance, token entropy, combined 'μ'), allows for AI and user quality ratings, and provides an interactive visualization of the relationships.\n",
        "\n",
        "## Features\n",
        "\n",
        "* Calculates DGFT-inspired metrics (μ, Variance, Entropy) for both prompts and responses.\n",
        "* Generates responses using configurable OpenAI models (e.g., GPT-4o, GPT-3.5-turbo).\n",
        "* Gets AI-driven quality scores for responses using configurable OpenAI models.\n",
        "* Allows users to interactively input their own quality scores (1-10).\n",
        "* Visualizes the relationship between prompt metrics (μ, entropy, variance) and response quality (AI or user score) using an interactive Plotly scatter plot.\n",
        "* Detailed tooltips show prompt/response text, metrics, and scores.\n",
        "* Configurable models for generation, rating, and embeddings via the sidebar.\n",
        "* Option to use a default list of prompts or enter custom prompts.\n",
        "\n",
        "## Setup and Running Locally\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```bash\n",
        "    git clone <your-repo-url>\n",
        "    cd your-repo-name\n",
        "    ```\n",
        "\n",
        "2.  **Create a virtual environment (recommended):**\n",
        "    ```bash\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install dependencies:**\n",
        "    ```bash\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "4.  **Set up OpenAI API Key:**\n",
        "    * **Option A (Environment Variable - Recommended):** Set the `OPENAI_API_KEY` environment variable. The app will pick it up.\n",
        "    * **Option B (Streamlit Secrets - Local):** Create a file `.streamlit/secrets.toml` (ensure this file is in your `.gitignore` and **never commit it!**) with the following content:\n",
        "        ```toml\n",
        "        OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
        "        ```\n",
        "    * **Option C (Direct Input):** Run the app and enter the key in the sidebar input field (less secure, not recommended for shared environments).\n",
        "\n",
        "5.  **Run the Streamlit app:**\n",
        "    ```bash\n",
        "    streamlit run streamlit_app.py\n",
        "    ```\n",
        "\n",
        "## Deployment to Streamlit Cloud\n",
        "\n",
        "1.  **Push your code to a GitHub repository.** Ensure your `secrets.toml` file is included in your `.gitignore` file and is NOT pushed to GitHub.\n",
        "2.  **Sign up or log in** to [Streamlit Community Cloud](https://streamlit.io/cloud).\n",
        "3.  Click \"**New app**\" and connect your GitHub account.\n",
        "4.  Select the repository and branch containing your `streamlit_app.py` and `requirements.txt` files.\n",
        "5.  **Configure Secrets:** In the advanced settings during deployment (or later in the app settings), add your `OPENAI_API_KEY` as a secret. The key name must match what's used in the code (`OPENAI_API_KEY`).\n",
        "6.  Click \"**Deploy!**\".\n",
        "\n",
        "## Usage\n",
        "\n",
        "1.  Ensure your OpenAI API key is configured (via sidebar input or secrets).\n",
        "2.  Select the desired OpenAI models for generation, rating, and embeddings in the sidebar.\n",
        "3.  Choose whether to use the default prompt list or enter your own (one per line).\n",
        "4.  Click the \"**Analyze Prompts and Generate Responses**\" button.\n",
        "5.  Wait for the analysis to complete (progress will be shown).\n",
        "6.  Review the results table.\n",
        "7.  Optionally, provide your **User Quality Ratings** (1-10) in the interactive section. Use 0 or leave blank to skip rating a specific response. Click \"**Apply User Ratings & Update Plot**\".\n",
        "8.  Explore the **Interactive Visualization**:\n",
        "    * Select whether the Y-axis represents AI or User Quality Score.\n",
        "    * Hover over points to see detailed information (prompt, response, metrics, scores).\n",
        "    * Zoom, pan, and interact with the Plotly chart.\n",
        "\n",
        "## DGFT-Inspired Metrics\n",
        "\n",
        "* **Embedding Variance:** Measures the spread or diversity of the concepts within the text's embedding vector. Lower variance might suggest more focused content.\n",
        "* **Token Entropy (Normalized):** Measures the predictability or repetitiveness of the token sequence based on Shannon entropy, normalized by the theoretical maximum for the sequence length. Lower entropy suggests more repetitive or predictable text.\n",
        "* **Combined μ (Mu):** Calculated as `exp(-(kv * variance + ke * entropy))`. This metric combines variance and entropy, where lower variance and lower entropy result in a higher μ (closer to 1), potentially indicating lower \"tension\" or higher conceptual/lexical coherence/simplicity. The weights `kv` and `ke` are tunable parameters (currently set to 50 and 1).\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This structure provides a self-contained Streamlit application deployable via GitHub, incorporates interactive elements for user rating, uses Streamlit's features for UI and state management (`st.session_state`), and handles API keys more appropriately for deployment using `st.secrets`. Remember to add `.streamlit/secrets.toml` to your `.gitignore` file!"
      ],
      "metadata": {
        "id": "tbaZMK18eNCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/Abthon/StuddyBuddy\">https://github.com/Abthon/StuddyBuddy</a></li>\n",
        "  <li><a href=\"https://github.com/ChukwumaKingsley/smart_school_forked\">https://github.com/ChukwumaKingsley/smart_school_forked</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "VKqCyGOKeNCx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}